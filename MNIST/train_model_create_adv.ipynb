{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow.keras.datasets import mnist, cifar10, cifar100\n",
    "\n",
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.callbacks import LambdaCallback\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Dropout, Dense, Flatten, Activation\n",
    "from tensorflow.keras.optimizers import SGD\n",
    "import numpy as np\n",
    "from numpy import save\n",
    "from numpy import asarray\n",
    "from numpy import load\n",
    "import random\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "labels = ['zero', 'one', 'two', 'three', 'four', 'five', 'six', 'seven', 'eight', 'nine']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data shapes (10000, 28, 28, 1) (10000, 10) (60000, 28, 28, 1) (60000, 10)\n"
     ]
    }
   ],
   "source": [
    "img_rows, img_cols, channels = 28, 28, 1\n",
    "num_classes = 10\n",
    "\n",
    "x_train = x_train.reshape((x_train.shape[0], img_rows, img_cols, channels))\n",
    "x_test = x_test.reshape((x_test.shape[0], img_rows, img_cols, channels))\n",
    "\n",
    "y_train = tf.keras.utils.to_categorical(y_train)\n",
    "y_test = tf.keras.utils.to_categorical(y_test)\n",
    "\n",
    "x_train = x_train.astype('float32')\n",
    "x_test = x_test.astype('float32')\n",
    "\n",
    "x_train = x_train / 255.0\n",
    "x_test = x_test / 255.0\n",
    "\n",
    "print(\"Data shapes\", x_test.shape, y_test.shape, x_train.shape, y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model():\n",
    "        model = Sequential()\n",
    "        model.add(Conv2D(32, (3, 3), padding='same', input_shape=(img_rows, img_cols, channels), activation='relu'))\n",
    "        model.add(Conv2D(32, (3, 3), activation='relu'))\n",
    "        model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "        model.add(Dropout(0.2))\n",
    "        model.add(Conv2D(64, (3, 3), padding='same', activation='relu'))\n",
    "        model.add(Conv2D(64, (3, 3), activation='relu'))\n",
    "        model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "        model.add(Dropout(0.2))\n",
    "        model.add(Conv2D(128, (3, 3), padding='same', activation='relu'))\n",
    "        model.add(Conv2D(128, (3, 3), activation='relu'))\n",
    "        model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "        model.add(Dropout(0.2))\n",
    "        model.add(Flatten())\n",
    "        model.add(Dense(512, activation='relu'))\n",
    "        model.add(Dropout(0.5))\n",
    "        model.add(Dense(num_classes, activation='softmax'))\n",
    "        sgd = SGD(lr=0.001, decay=1e-6, momentum=0.9, nesterov=True)\n",
    "        model.compile(loss='categorical_crossentropy', optimizer=sgd, metrics=['accuracy'])\n",
    "#     model = Sequential()\n",
    "#     model.add(Conv2D(32, kernel_size=(3, 3), strides=(3, 3), padding='same', activation='relu', input_shape=(img_rows, img_cols, channels)))\n",
    "#     model.add(Conv2D(64, kernel_size=(3, 3), strides=(3, 3), padding='same', activation='relu'))\n",
    "#     model.add(Conv2D(64, kernel_size=(3, 3), strides=(3, 3), padding='same', activation='relu'))\n",
    "#     model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "#     model.add(Dropout(0.2))\n",
    "#     model.add(Flatten())\n",
    "#     model.add(Dense(32))\n",
    "#     model.add(Dropout(0.2))\n",
    "#     model.add(Dense(32))\n",
    "#     model.add(Dropout(0.2))\n",
    "#     model.add(Dense(num_classes, activation='softmax'))\n",
    "#     model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "        return model\n",
    "\n",
    "model = create_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/10\n",
      "60000/60000 [==============================] - 24s 407us/sample - loss: 1.3176 - accuracy: 0.5378 - val_loss: 0.1951 - val_accuracy: 0.9393\n",
      "Epoch 2/10\n",
      "60000/60000 [==============================] - 35s 578us/sample - loss: 0.2309 - accuracy: 0.9294 - val_loss: 0.0898 - val_accuracy: 0.9725\n",
      "Epoch 3/10\n",
      "60000/60000 [==============================] - 39s 654us/sample - loss: 0.1422 - accuracy: 0.9571 - val_loss: 0.0631 - val_accuracy: 0.9795\n",
      "Epoch 4/10\n",
      "60000/60000 [==============================] - 39s 655us/sample - loss: 0.1113 - accuracy: 0.9666 - val_loss: 0.0475 - val_accuracy: 0.9849\n",
      "Epoch 5/10\n",
      "60000/60000 [==============================] - 39s 651us/sample - loss: 0.0907 - accuracy: 0.9730 - val_loss: 0.0396 - val_accuracy: 0.9877\n",
      "Epoch 6/10\n",
      "60000/60000 [==============================] - 39s 657us/sample - loss: 0.0783 - accuracy: 0.9763 - val_loss: 0.0349 - val_accuracy: 0.9891\n",
      "Epoch 7/10\n",
      "60000/60000 [==============================] - 39s 656us/sample - loss: 0.0708 - accuracy: 0.9787 - val_loss: 0.0316 - val_accuracy: 0.9894\n",
      "Epoch 8/10\n",
      "60000/60000 [==============================] - 40s 659us/sample - loss: 0.0634 - accuracy: 0.9816 - val_loss: 0.0308 - val_accuracy: 0.9890\n",
      "Epoch 9/10\n",
      "60000/60000 [==============================] - 39s 652us/sample - loss: 0.0580 - accuracy: 0.9818 - val_loss: 0.0274 - val_accuracy: 0.9903\n",
      "Epoch 10/10\n",
      "60000/60000 [==============================] - 54s 905us/sample - loss: 0.0553 - accuracy: 0.9835 - val_loss: 0.0240 - val_accuracy: 0.9924\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7fcdd804f0b8>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(x_train, y_train,\n",
    "          batch_size=32,\n",
    "          epochs=10,\n",
    "          validation_data=(x_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/10\n",
      "60000/60000 [==============================] - 54s 906us/sample - loss: 1.3823 - accuracy: 0.5194 - val_loss: 0.1868 - val_accuracy: 0.9418\n",
      "Epoch 2/10\n",
      "60000/60000 [==============================] - 54s 900us/sample - loss: 0.2368 - accuracy: 0.9279 - val_loss: 0.0894 - val_accuracy: 0.9702\n",
      "Epoch 3/10\n",
      "60000/60000 [==============================] - 53s 878us/sample - loss: 0.1493 - accuracy: 0.9558 - val_loss: 0.0704 - val_accuracy: 0.9778\n",
      "Epoch 4/10\n",
      "60000/60000 [==============================] - 54s 902us/sample - loss: 0.1125 - accuracy: 0.9659 - val_loss: 0.0515 - val_accuracy: 0.9839\n",
      "Epoch 5/10\n",
      "60000/60000 [==============================] - 53s 884us/sample - loss: 0.0958 - accuracy: 0.9719 - val_loss: 0.0458 - val_accuracy: 0.9845\n",
      "Epoch 6/10\n",
      "60000/60000 [==============================] - 54s 902us/sample - loss: 0.0807 - accuracy: 0.9757 - val_loss: 0.0384 - val_accuracy: 0.9875\n",
      "Epoch 7/10\n",
      "60000/60000 [==============================] - 53s 889us/sample - loss: 0.0712 - accuracy: 0.9791 - val_loss: 0.0322 - val_accuracy: 0.9894\n",
      "Epoch 8/10\n",
      "60000/60000 [==============================] - 54s 895us/sample - loss: 0.0634 - accuracy: 0.9807 - val_loss: 0.0305 - val_accuracy: 0.9899\n",
      "Epoch 9/10\n",
      "60000/60000 [==============================] - 53s 884us/sample - loss: 0.0587 - accuracy: 0.9820 - val_loss: 0.0277 - val_accuracy: 0.9903\n",
      "Epoch 10/10\n",
      "60000/60000 [==============================] - 54s 902us/sample - loss: 0.0532 - accuracy: 0.9839 - val_loss: 0.0293 - val_accuracy: 0.9911\n"
     ]
    }
   ],
   "source": [
    "m1=tf.keras.models.clone_model(model)\n",
    "sgd = SGD(lr=0.001, decay=1e-6, momentum=0.9, nesterov=True)\n",
    "m1.compile(loss='categorical_crossentropy', optimizer=sgd, metrics=['accuracy'])\n",
    "# m1.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "m1.fit(x_train, y_train,\n",
    "          batch_size=32,\n",
    "          epochs=10,\n",
    "          validation_data=(x_test, y_test))\n",
    "m1.save('final_model_clean.h5')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Base accuracy on regular images: 0.9924\n",
      "Base loss on regular images: 0.024030854113490204\n"
     ]
    }
   ],
   "source": [
    "loss, acc = model.evaluate(x_test, y_test, verbose=0)\n",
    "print(\"Base accuracy on regular images:\", acc)\n",
    "print(\"Base loss on regular images:\", loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def adversarial_pattern(image, label):\n",
    "    image = tf.cast(image, tf.float32)\n",
    "    \n",
    "    with tf.GradientTape() as tape:\n",
    "        tape.watch(image)\n",
    "        prediction = model(image)\n",
    "        loss = tf.keras.losses.categorical_crossentropy(label, prediction)\n",
    "    \n",
    "    gradient = tape.gradient(loss, image)\n",
    "    \n",
    "    signed_grad = tf.sign(gradient)\n",
    "    \n",
    "    return signed_grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "image = x_train[0]\n",
    "image_label = y_train[0]\n",
    "perturbations = adversarial_pattern(image.reshape((1, img_rows, img_cols, channels)), image_label).numpy()\n",
    "adversarial = image + perturbations * 0.2\n",
    "if channels == 1:\n",
    "    plt.imshow(adversarial.reshape((img_rows, img_cols)))\n",
    "else:\n",
    "    plt.imshow(adversarial.reshape((img_rows, img_cols, channels)))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(labels[model.predict(image.reshape((1, img_rows, img_cols, channels))).argmax()])\n",
    "# print(labels[model.predict(adversarial).argmax()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def generate_adversarials(batch_size):\n",
    "    while True:\n",
    "        x = []\n",
    "        y = []\n",
    "        for batch in range(batch_size):\n",
    "            #N = random.randint(0, 100)\n",
    "\n",
    "            label = y_test[batch]\n",
    "            image = x_test[batch]\n",
    "            \n",
    "            perturbations = adversarial_pattern(image.reshape((1, img_rows, img_cols, channels)), label).numpy()\n",
    "            \n",
    "            \n",
    "            epsilon = 0.3\n",
    "            adversarial = image + perturbations * epsilon\n",
    "            \n",
    "            x.append(adversarial)\n",
    "            y.append(y_test[batch])\n",
    "        \n",
    "        \n",
    "        x = np.asarray(x).reshape((batch_size, img_rows, img_cols, channels))\n",
    "        y = np.asarray(y)\n",
    "        \n",
    "        yield x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_adversarial_test, y_adversarial_test = next(generate_adversarials(10000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Base accuracy on regular images: 0.9924\n",
      "Base loss on regular images: 0.024030854113490204\n",
      "Base accuracy on adversarial images: 0.1189\n",
      "Base loss on adversarial images: 4.465695142364502\n"
     ]
    }
   ],
   "source": [
    "loss1, acc1 = model.evaluate(x_test, y_test, verbose=0)\n",
    "print(\"Base accuracy on regular images:\", acc1)\n",
    "print(\"Base loss on regular images:\", loss1)\n",
    "loss2, acc2 = model.evaluate(x_adversarial_test, y_adversarial_test, verbose=0)\n",
    "print(\"Base accuracy on adversarial images:\",acc2)\n",
    "print('Base loss on adversarial images:', loss2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "save('adversarial_test_images',x_adversarial_test)\n",
    "save('adversarial_test_labels',y_adversarial_test)\n",
    "save('clean_test_images',x_test)\n",
    "save('clean_test_labels',y_test)\n",
    "model.save('final_model_adv.h5')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
